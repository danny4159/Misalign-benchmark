_target_: misalign.models.proposed_module_A_to_B.ProposedModule_A_to_B

name: proposed_A_to_B

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.0001
  betas: [0.5, 0.999]
  weight_decay: 0.0001

netG_A:
  _target_: misalign.models.components.networks.define_G
  input_nc: 1
  output_nc: 1
  ngf: 64
  norm: 'instance'
  use_dropout: False
  init_type: 'normal'

netD_A:
  _target_: misalign.models.components.networks.define_D
  input_nc: 1
  ndf: 64
  norm: 'instance'
  n_layers_D: 3
  init_type: 'normal'

params: # Other params
  pool_size: 0
  lambda_l1: 20 # L1 loss
  lambda_style: 20 # Style loss
  lambda_smooth: 10 # L1-Smooth loss
  lambda_reg_consistency: 10 # Reg 를 끄는 방법 -> 0
  reverse: ${data.reverse} # A->B if False, B->A if True
  flag_GAN: True # Else : no GAN
  flag_register: True # else l1  -> False: Registration 꺼짐
  flag_ctx: True # Else: l1-perceptual # False: CTX 가 아니고 Perceptual
  flag_feature_descriptor: VGG # or VGG
  flag_meta_learning: True # else no meta # 끄면 Meta x
  flag_meta_use_spatial: True # 혹시 시간 되면 True False 의 차이 
  meta_type: LRE # 고정
  flag_use_mask: False # use mask in meta loss # 혹시 시간 되면 True False 의 차이 
  # dir_meta_learning: ${paths.root_dir}/data/IXI/val/prepared_data_0.0_0.0_0.0_0.0_0.0.h5 #TODO: 수정
  dir_meta_learning: ${paths.root_dir}/data/SynthRAD_MR_CT_Pelvis/val/prepared_data_0_0_0_0_0.h5 #TODO: 수정