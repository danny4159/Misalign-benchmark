{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd2aec7a-ec7a-4585-8cb1-e2efcd0b03b0",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4339779e-6cf4-432c-bd81-cf246d00d019",
   "metadata": {},
   "source": [
    "```Objective```\n",
    "\n",
    "Change the generators to be same as diffusion Unet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f4dfe32-a227-4923-ab64-63854419ca95",
   "metadata": {},
   "source": [
    "```Methods```\n",
    "\n",
    "todo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c19ae45-56fd-4446-a49f-29fcc05bc885",
   "metadata": {},
   "source": [
    "```Results```\n",
    "\n",
    "todo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10534042-e2b9-4547-a8e0-d7ad2d85f169",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45dfab17-bead-4526-8257-83b2acbe13c0",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b27ade-f6fb-45b5-be5e-147297af867e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fac5eaa-e0cd-4282-b9ad-c4e126363902",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42d698c2-8c1b-4f83-8f52-9e94d8435bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \n",
    "    def __init__(self, C):\n",
    "        \"\"\"\n",
    "        :param C (int): number of input and output channels\n",
    "        \"\"\"\n",
    "        super(Downsample, self).__init__()\n",
    "        self.conv = nn.Conv2d(C, C, 3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.conv(x)\n",
    "        assert x.shape == (B, C, H // 2, W // 2)\n",
    "        return x\n",
    "    \n",
    "class Upsample(nn.Module):\n",
    "    \n",
    "    def __init__(self, C):\n",
    "        \"\"\"\n",
    "        :param C (int): number of input and output channels\n",
    "        \"\"\"\n",
    "        super(Upsample, self).__init__()\n",
    "        self.conv = nn.Conv2d(C, C, 3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        x = nn.functional.interpolate(x, size=None, scale_factor=2, mode='nearest')\n",
    "  \n",
    "        x = self.conv(x)\n",
    "        assert x.shape == (B, C, H * 2, W * 2)\n",
    "        return x\n",
    "    \n",
    "class Nin(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, scale = 1e-10):\n",
    "        super(Nin, self).__init__()\n",
    "        \n",
    "        n = (in_dim + out_dim) / 2\n",
    "        limit = np.sqrt(3 * scale / n)\n",
    "        self.W = torch.nn.Parameter(torch.zeros((in_dim, out_dim), dtype=torch.float32\n",
    "                                               ).uniform_(-limit, limit))\n",
    "        self.b = torch.nn.Parameter(torch.zeros((1, out_dim, 1, 1), dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):    \n",
    "        return torch.einsum('bchw, co->bowh', x, self.W) + self.b\n",
    "    \n",
    "class ResNetBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch, dropout_rate=0.0):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1)\n",
    "        \n",
    "        if not (in_ch == out_ch):\n",
    "            self.nin = Nin(in_ch, out_ch)\n",
    "            \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nonlinearity = torch.nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        h = self.nonlinearity(nn.functional.instance_norm(x))\n",
    "        h = self.conv1(h)\n",
    "        h = self.nonlinearity(nn.functional.instance_norm(h))\n",
    "        h = nn.functional.dropout(h, p=self.dropout_rate)\n",
    "        h = self.conv2(h)\n",
    "        \n",
    "        if not (x.shape[1] == h.shape[1]):\n",
    "            x = self.nin(x)\n",
    "            \n",
    "        assert x.shape == h.shape\n",
    "        return x + h\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, ch):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        \n",
    "        self.Q = Nin(ch, ch)\n",
    "        self.K = Nin(ch, ch)\n",
    "        self.V = Nin(ch, ch)\n",
    "        \n",
    "        self.ch = ch\n",
    "        \n",
    "        self.nin = Nin(ch, ch, scale=0.)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        assert C == self.ch\n",
    "        \n",
    "        h = nn.functional.group_norm(x, num_groups=32)\n",
    "        q = self.Q(h)\n",
    "        k = self.K(h)\n",
    "        v = self.V(h)\n",
    "        \n",
    "        w = torch.einsum('bchw,bcHW->bhwHW', q, k) * (int(C) ** (-0.5)) # [B, H, W, H, W]\n",
    "        w = torch.reshape(w, [B, H, W, H * W])\n",
    "        w = torch.nn.functional.softmax(w, dim=-1)\n",
    "        w = torch.reshape(w, [B, H, W, H, W])\n",
    "        \n",
    "        h = torch.einsum('bhwHW,bcHW->bchw', w, v)\n",
    "        h = self.nin(h)\n",
    "        \n",
    "        assert h.shape == x.shape\n",
    "        return x + h\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_nc=1, output_nc=1, ngf=128, *args, **kwargs):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.ch = ngf\n",
    "        ch = ngf\n",
    "        self.conv1 = nn.Conv2d(input_nc, ch, 3, stride=1, padding=1)\n",
    "        self.down = nn.ModuleList([ResNetBlock(ch, 1 * ch),\n",
    "                                   ResNetBlock(1 * ch, 1 * ch),\n",
    "                                   Downsample(1 * ch),\n",
    "                                   ResNetBlock(1 * ch, 2 * ch),\n",
    "                                   ResNetBlock(2 * ch, 2 * ch),\n",
    "                                   Downsample(2 * ch),\n",
    "                                   ResNetBlock(2 * ch, 2 * ch),\n",
    "                                   ResNetBlock(2 * ch, 2 * ch),\n",
    "                                   Downsample(2 * ch),\n",
    "                                   ResNetBlock(2 * ch, 2 * ch),\n",
    "                                   ResNetBlock(2 * ch, 2 * ch)])\n",
    "        \n",
    "        self.middle = nn.ModuleList([ResNetBlock(2 * ch, 2 * ch),\n",
    "                                     ResNetBlock(2 * ch, 2 * ch)])\n",
    "        \n",
    "        self.up = nn.ModuleList([ResNetBlock(4 * ch, 2 * ch),\n",
    "                                 ResNetBlock(4 * ch, 2 * ch),\n",
    "                                 ResNetBlock(4 * ch, 2 * ch),\n",
    "                                 Upsample(2 * ch),\n",
    "                                 ResNetBlock(4 * ch, 2 * ch),\n",
    "                                 ResNetBlock(4 * ch, 2 * ch),\n",
    "                                 ResNetBlock(4 * ch, 2 * ch),\n",
    "                                 Upsample(2 * ch),\n",
    "                                 ResNetBlock(4 * ch, 2 * ch),\n",
    "                                 AttentionBlock(2 * ch),\n",
    "                                 ResNetBlock(4 * ch, 2 * ch),\n",
    "                                 AttentionBlock(2 * ch),\n",
    "                                 ResNetBlock(3 * ch, 2 * ch),\n",
    "                                 AttentionBlock(2 * ch),\n",
    "                                 Upsample(2 * ch),\n",
    "                                 ResNetBlock(3 * ch, ch),\n",
    "                                 ResNetBlock(2 * ch, ch),\n",
    "                                 ResNetBlock(2 * ch, ch)]) \n",
    "        \n",
    "        self.final_conv = nn.Conv2d(ch, output_nc, 3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x (torch.Tensor): batch of images [B, C, H, W]\n",
    "        :param t (torch.Tensor): tensor of time steps (torch.long) [B]\n",
    "        \"\"\"\n",
    "        x1 = self.conv1(x)\n",
    "\n",
    "        # Down\n",
    "        x2 = self.down[0](x1)\n",
    "        x3 = self.down[1](x2)\n",
    "        x4 = self.down[2](x3)\n",
    "        x5 = self.down[3](x4)\n",
    "        x6 = self.down[4](x5)  \n",
    "        x7 = self.down[5](x6)\n",
    "        x8 = self.down[6](x7)\n",
    "        x9 = self.down[7](x8)\n",
    "        x10 = self.down[8](x9)\n",
    "        x11 = self.down[9](x10)\n",
    "        x12 = self.down[10](x11)\n",
    "        \n",
    "        # Middle\n",
    "        x = self.middle[0](x12)\n",
    "        x = self.middle[1](x)\n",
    "        \n",
    "        # Up\n",
    "        x = self.up[0](torch.cat((x, x12), dim=1))\n",
    "        x = self.up[1](x)\n",
    "        x = self.up[2](torch.cat((x, x11), dim=1))\n",
    "        x = self.up[3](torch.cat((x, x10), dim=1))\n",
    "        x = self.up[4](x)\n",
    "        x = self.up[5](torch.cat((x, x9), dim=1))\n",
    "        x = self.up[6](x)\n",
    "        x = self.up[7](torch.cat((x, x8), dim=1))\n",
    "        x = self.up[8](torch.cat((x, x7), dim=1))\n",
    "        x = self.up[9](x)\n",
    "        x = self.up[10](torch.cat((x, x6), dim=1))\n",
    "        x = self.up[11](x)\n",
    "        x = self.up[12](torch.cat((x, x5), dim=1))\n",
    "        x = self.up[13](x)\n",
    "        x = self.up[14](torch.cat((x, x4), dim=1))\n",
    "        x = self.up[15](x)\n",
    "        x = self.up[16](x)\n",
    "        x = self.up[17](torch.cat((x, x3), dim=1))\n",
    "        x = self.up[18](torch.cat((x, x2), dim=1))\n",
    "        x = self.up[19](torch.cat((x, x1), dim=1))\n",
    "        \n",
    "        x = nn.functional.silu(nn.functional.instance_norm(x))\n",
    "        x = self.final_conv(x)\n",
    "        x = nn.functional.tanh(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb61c119-06df-4bc8-8e29-62185d85d927",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c4ffa1-60d0-45c2-b65a-25adaf0161e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(in_channels=1, out_channels=1, ch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a0939a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m out \u001b[39m=\u001b[39m unet(test)\n",
      "File \u001b[0;32m~/miniconda3/envs/misalign/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 189\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39m# Up\u001b[39;00m\n\u001b[1;32m    188\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup[\u001b[39m0\u001b[39m](torch\u001b[39m.\u001b[39mcat((x, x12), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 189\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup[\u001b[39m1\u001b[39;49m](x)\n\u001b[1;32m    190\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup[\u001b[39m2\u001b[39m](torch\u001b[39m.\u001b[39mcat((x, x11), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    191\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup[\u001b[39m3\u001b[39m](torch\u001b[39m.\u001b[39mcat((x, x10), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/misalign/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 99\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     98\u001b[0m     B, C, H, W \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m---> 99\u001b[0m     \u001b[39massert\u001b[39;00m C \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mch\n\u001b[1;32m    101\u001b[0m     h \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mgroup_norm(x, num_groups\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[1;32m    102\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ(h)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = torch.randn(1, 1, 256, 256)\n",
    "\n",
    "out = unet(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
